import os
import sys
import pandas as pd
from moviepy.editor import VideoFileClip
from transformers import AutoProcessor, Qwen2_5_VLForConditionalGeneration
import torch
from PIL import Image
import numpy as np
import cv2

PROJECT_ROOT = os.path.abspath(os.path.join(
    os.path.dirname(__file__), '..'
))
sys.path.append(PROJECT_ROOT)
import config as cfg
from helper_functions import list_recordings

def sample_frames_with_opencv(video_path: str, num_frames: int) -> list:
    """
    Samples frames uniformly from a video using OpenCV.

    Args:
        video_path: Path to the video file.
        num_frames: The number of frames to sample.

    Returns:
        A list of PIL Image objects.
    """
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        raise IOError(f"Cannot open video file: {video_path}")
    
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    if total_frames < num_frames:
        print(f"Warning: Video has fewer frames ({total_frames}) than requested ({num_frames}). Using all frames.")
        num_frames = total_frames
        
    indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)
    
    frames = []
    for idx in indices:
        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)
        ret, frame = cap.read()
        if ret:
            # OpenCV reads frames in BGR format, convert to RGB for PIL
            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            frames.append(Image.fromarray(rgb_frame))
            
    cap.release()
    return frames


# 1. Global cache to store loaded models and processors
LOADED_MODELS = {}

def get_vlm_video_label(video_chunk_path: str, prompt: str, model_name: str, num_frames: int = 4) -> str:
    """
    Generates a label for a video chunk by sampling multiple frames and using a VLM.

    This function will:
    1. Check if the specified model is already loaded; if not, it loads it.
    2. Sample `num_frames` evenly from the video file using decord.
    3. Send the list of frames and the prompt to the VLM.
    4. Return the text label generated by the model.
    """

    if model_name not in LOADED_MODELS:
        print(f"Model '{model_name}' not found in cache. Loading now...")
        model_id = "Qwen/Qwen2.5-VL-7B-Instruct"
        if "qwen" in model_name.lower():
            model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
                    model_id,
                    dtype=torch.bfloat16,          
                    device_map="auto",             
                    attn_implementation="sdpa"     
                ).eval()
            
        elif "smolvlm" in model_name.lower():
            model_id = "HuggingFaceTB/SmolVLM2-500M-Video-Instruct"
            pass
        else:
            raise ValueError("Unsupported model_name. Use 'qwen' or 'smolvlm'.")

        processor = AutoProcessor.from_pretrained(model_id)


        device = cfg.DEVICE

        LOADED_MODELS[model_name] = (model, processor, device)
        print(f"Model '{model_name}' loaded successfully on {device}.")
    else:
        print(f"Using cached model: '{model_name}'")

    model, processor, device = LOADED_MODELS[model_name]

    # --- Step 2: Sample frames from the video using decord ---
    try:
        video_frames = sample_frames_with_opencv(video_chunk_path, num_frames)
    except Exception as e:
        return f"Error processing video file with decord: {e}"

    # --- Step 3: Prepare inputs and run inference ---
    print(f"Processing {len(video_frames)} frames from: {os.path.basename(video_chunk_path)}")
    
    messages = [
        {
            "role": "user",
            "content": [
                {"type": "video", "video": video_frames, "fps": 4.0}, 
                {"type": "text", "text": prompt}
            ]
        }
    ]

    # Use the special `qwen-vl-utils` for Qwen if available, otherwise use processor
    try:
        text_prompt = processor.apply_chat_template(messages, add_generation_prompt=True)
        inputs = processor(text=[text_prompt], videos=[video_frames], return_tensors="pt").to(device)
    except Exception:
        text_prompt = processor.apply_chat_template(messages, add_generation_prompt=True)
        inputs = processor(text=text_prompt, images=video_frames, return_tensors="pt").to(device)


    # Generate a response
    generated_ids = model.generate(**inputs, max_new_tokens=64)
    generated_ids = generated_ids[:, inputs['input_ids'].shape[1]:]

    response = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()
    return response

def split_recordings():
    """
    Splits all recordings (CSV and MP4 files) into smaller chunks
    based on the duration specified in the config file.
    """
    print(">>> Starting the splitting process...")
    source_folder = cfg.RECORDINGS_FOLDER
    output_folder = cfg.SPLITTED_RECORDINGS_FOLDER
    split_duration = cfg.VIDEO_SPLIT_DURATION

    os.makedirs(output_folder, exist_ok=True)
    
    all_recordings = list_recordings()
    if not all_recordings:
        print("No recordings found to split.")
        return

    chunk_id_counter = 0

    unique_prefixes = sorted(list(set([f.split('_')[0] for f in all_recordings])))

    for prefix in unique_prefixes:
        # Find the latest CSV and MP4 for this prefix
        csv_files = [f for f in all_recordings if f.startswith(f"{prefix}_") and f.endswith(".csv")]
        mp4_files = [f.replace('.csv', '.mp4') for f in csv_files]
        
        if not csv_files:
            continue
            
        latest_csv = max(csv_files, key=lambda f: os.path.getmtime(os.path.join(source_folder, f)))
        latest_mp4 = latest_csv.replace('.csv', '.mp4')
        
        csv_path = os.path.join(source_folder, latest_csv)
        video_path = os.path.join(source_folder, latest_mp4)

        if not os.path.exists(video_path):
            print(f"Warning: Video file not found for {latest_csv}, skipping this pair.")
            continue

        print(f"\nProcessing recording: {os.path.basename(csv_path)}")

        # --- Split Video File ---
        video = VideoFileClip(video_path)
        total_duration = video.duration
        
        start_time = 0
        while start_time < total_duration:
            end_time = min(start_time + split_duration, total_duration)
            
            # Define output paths for the new chunks
            video_chunk_path = os.path.join(output_folder, f"{chunk_id_counter}.mp4")
            csv_chunk_path = os.path.join(output_folder, f"{chunk_id_counter}.csv")

            print(f"- Creating chunk {chunk_id_counter} ({start_time:.2f}s to {end_time:.2f}s)")

            # Create and save the video subclip
            subclip = video.subclip(start_time, end_time)
            subclip.write_videofile(video_chunk_path, codec="libx264", audio=False, logger=None)

            # --- Split CSV File ---
            df = pd.read_csv(csv_path)
            # Filter rows where 'Time' is within the current chunk's interval
            time_mask = (df['Time'] >= start_time) & (df['Time'] < end_time)
            chunk_df = df[time_mask]
            
            if not chunk_df.empty:
                chunk_df.to_csv(csv_chunk_path, index=False)

            start_time += split_duration
            chunk_id_counter += 1
            
        video.close()
    
    print(f"\n>>> Splitting complete. Created {chunk_id_counter} chunks.")


def label_recordings():
    """
    Labels each video chunk in the splitted_recordings folder using a VLM
    and saves the results to a labels.csv file.
    """
    print("\n>>> Starting the labeling process...")
    source_folder = cfg.SPLITTED_RECORDINGS_FOLDER
    prompt = cfg.VLM_PROMPT
    
    video_chunks = sorted(
        [f for f in os.listdir(source_folder) if f.endswith('.mp4')],
        key=lambda x: int(os.path.splitext(x)[0]) # Sort numerically
    )
    
    if not video_chunks:
        print("No video chunks found to label.")
        return
        
    results = []
    for video_file in video_chunks:
        video_path = os.path.join(source_folder, video_file)
        chunk_id = int(os.path.splitext(video_file)[0])
        
        # Get the label from the VLM
        label = get_vlm_video_label(video_path, prompt, cfg.VLM_MODEL)
        
        results.append({'id': chunk_id, 'label': label})

    # Save the results to a master CSV file
    output_csv_path = os.path.join(source_folder, 'labels.csv')
    labels_df = pd.DataFrame(results)
    labels_df.to_csv(output_csv_path, index=False)
    
    print(f"\n>>> Labeling complete. Results saved to {output_csv_path}")


if __name__ == "__main__":
    if not os.path.exists(cfg.SPLITTED_RECORDINGS_FOLDER):
        print("Splitted recordings folder not found. Running the splitting process first.")
        split_recordings()
    else:
        print("Splitted recordings folder already exists. Skipping the splitting process.")

    label_recordings()